version: '3.8'

networks:
  optima-llm-network:
    name: ${NETWORK_NAME:-optima-llm-network}

services:
  llama-cpp-server:
    container_name: ${LLAMA_CPP_SERVER_NAME:-llama-cpp-server}
    image: ghcr.io/ggerganov/llama.cpp:full
    restart: unless-stopped
    environment:
      - MODEL=${MODEL_PATH:-/models/gemma-3-12b-instruct.Q4_K_M.gguf}
      - CONTEXT_SIZE=${CONTEXT_SIZE:-8192}
      - BATCH_SIZE=${BATCH_SIZE:-512}
      - THREADS=${THREADS:-8}
      - N_GPU_LAYERS=${N_GPU_LAYERS:--1}
      - USE_MLOCK=${USE_MLOCK:-true}
      - MAIN_GPU=${MAIN_GPU:-0}
      # Inference parameters
      - TEMPERATURE=${TEMPERATURE:-0.7}
      - TOP_P=${TOP_P:-0.9}
      - REPEAT_PENALTY=${REPEAT_PENALTY:-1.1}
      - FREQUENCY_PENALTY=${FREQUENCY_PENALTY:-0.0}
      - PRESENCE_PENALTY=${PRESENCE_PENALTY:-0.0}
    volumes:
      - ${MODELS_DIR:-./models}:/models
    ports:
      - "${LLAMA_CPP_PORT:-8080}:8080"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [compute, utility]
              device_ids: ["${NVIDIA_VISIBLE_DEVICES:-all}"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 20s
    networks:
      - optima-llm-network

  open-webui:
    container_name: ${WEBUI_NAME:-open-webui}
    image: ghcr.io/open-webui/open-webui:main
    restart: unless-stopped
    environment:
      - OLLAMA_BASE_URL=http://${LLAMA_CPP_SERVER_NAME:-llama-cpp-server}:8080
    volumes:
      - ${WEBUI_DATA_DIR:-./webui-data}:/app/backend/data
    ports:
      - "${WEBUI_PORT:-3000}:3000"
    depends_on:
      llama-cpp-server:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 20s
    networks:
      - optima-llm-network