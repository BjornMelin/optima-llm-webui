# Container names
LLAMA_CPP_SERVER_NAME=llama-cpp-server
WEBUI_NAME=open-webui

# Network settings
NETWORK_NAME=optima-llm-network
LLAMA_CPP_PORT=8080
WEBUI_PORT=3000

# Host paths (edit these to match your setup)
MODELS_DIR=./models
CONFIG_DIR=./config
WEBUI_DATA_DIR=./webui-data

# Model settings (these will be updated by the model manager)
MODEL_PATH=/models/gemma-3-12b-instruct.Q4_K_M.gguf
CONTEXT_SIZE=8192
BATCH_SIZE=512
THREADS=8
N_GPU_LAYERS=-1
USE_MLOCK=true

# Inference settings
TEMPERATURE=0.7
TOP_P=0.9
FREQUENCY_PENALTY=0.0
PRESENCE_PENALTY=0.0
REPEAT_PENALTY=1.1

# Docker settings
NVIDIA_VISIBLE_DEVICES=all

# Multi-modal settings (will be uncommented when using multimodal models)
# MULTIMODAL=true
# VISION_TOWER_PATH=/models/vision/phi-4-multimodal/vision_tower.bin
# MMPROJ_PATH=/models/mmproj/phi-4-multimodal/mmproj.bin